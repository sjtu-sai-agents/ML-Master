{
    "title": "chaii-hindi-and-tamil-question-answering",
    "steps": [
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> python ml_master.py --task chaii-hindi-and-tamil-question-answering --time-limit 12h",
            "delay": 200
        },
        {
            "text": "<span class='info'>[INFO]</span>: Starting run \"chaii-hindi-and-tamil-question-answering\"",
            "delay": 200
        },
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> ",
            "delay": 200
        }
    ],
    "code": "<span class=\"keyword\">import</span> os\n\nos.makedirs(<span class=\"string\">&quot;</span><span class=\"string\">./submission</span><span class=\"string\">&quot;</span>, exist_ok=<span class=\"keyword\">True</span>)\n<span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd\n<span class=\"keyword\">import</span> torch\n<span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np\n<span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split\n<span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    default_data_collator,\n    get_cosine_schedule_with_warmup,\n)\n<span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader\n<span class=\"keyword\">from</span> torch.optim <span class=\"keyword\">import</span> AdamW\n<span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm\n\n<span class=\"comment\"># Load data</span>\ntrain_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/train.csv</span><span class=\"string\">&quot;</span>)\ntest_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/test.csv</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Optimized model selection</span>\nmodel_name = <span class=\"string\">&quot;</span><span class=\"string\">deepset/xlm-roberta-large-squad2</span><span class=\"string\">&quot;</span>\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name).to(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span>)\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">preprocess_data</span>(df):\n    processed = []\n    <span class=\"keyword\">for</span> idx, row in df.iterrows():\n        context = row[<span class=\"string\">&quot;</span><span class=\"string\">context</span><span class=\"string\">&quot;</span>]\n        question = row[<span class=\"string\">&quot;</span><span class=\"string\">question</span><span class=\"string\">&quot;</span>]\n        answer = row.get(<span class=\"string\">&quot;</span><span class=\"string\">answer_text</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">&quot;</span>)\n\n        encodings = tokenizer(\n            question,\n            context,\n            truncation=<span class=\"string\">&quot;</span><span class=\"string\">only_second</span><span class=\"string\">&quot;</span>,\n            max_length=<span class=\"number\">512</span>,\n            stride=<span class=\"number\">128</span>,\n            return_overflowing_tokens=<span class=\"keyword\">True</span>,\n            return_offsets_mapping=<span class=\"keyword\">True</span>,\n            padding=<span class=\"string\">&quot;</span><span class=\"string\">max_length</span><span class=\"string\">&quot;</span>,\n        )\n\n        <span class=\"keyword\">for</span> i in range(len(encodings[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>])):\n            offset_mapping = encodings[<span class=\"string\">&quot;</span><span class=\"string\">offset_mapping</span><span class=\"string\">&quot;</span>][i]\n            start_pos = <span class=\"number\">0</span>\n            end_pos = <span class=\"number\">0</span>\n\n            <span class=\"keyword\">if</span> answer:\n                answer_start = row[<span class=\"string\">&quot;</span><span class=\"string\">answer_start</span><span class=\"string\">&quot;</span>]\n                answer_end = answer_start + len(answer)\n                <span class=\"keyword\">for</span> token_idx, (start, end) in enumerate(offset_mapping):\n                    <span class=\"keyword\">if</span> start &lt;= answer_start &lt; end:\n                        start_pos = token_idx\n                    <span class=\"keyword\">if</span> start &lt; answer_end &lt;= end:\n                        end_pos = token_idx\n                <span class=\"keyword\">if</span> end_pos == <span class=\"number\">0</span>:\n                    end_pos = start_pos\n\n            processed.append(\n                {\n                    <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: encodings[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>][i],\n                    <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: encodings[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>][i],\n                    <span class=\"string\">&quot;</span><span class=\"string\">start_positions</span><span class=\"string\">&quot;</span>: start_pos,\n                    <span class=\"string\">&quot;</span><span class=\"string\">end_positions</span><span class=\"string\">&quot;</span>: end_pos,\n                    <span class=\"string\">&quot;</span><span class=\"string\">original_idx</span><span class=\"string\">&quot;</span>: idx,\n                }\n            )\n    <span class=\"keyword\">return</span> processed\n\n\ntrain_data = preprocess_data(train_df)\noriginal_indices = [item[<span class=\"string\">&quot;</span><span class=\"string\">original_idx</span><span class=\"string\">&quot;</span>] <span class=\"keyword\">for</span> item in train_data]\nlanguages = train_df.iloc[original_indices][<span class=\"string\">&quot;</span><span class=\"string\">language</span><span class=\"string\">&quot;</span>].tolist()\n\ntrain_set, val_set = train_test_split(\n    train_data, test_size=<span class=\"number\">0.1</span>, random_state=<span class=\"number\">42</span>, stratify=languages\n)\n\n\n<span class=\"keyword\">class</span> <span class=\"class\">QADataset</span>(Dataset):\n    <span class=\"keyword\">def</span> <span class=\"function\">__init__</span>(self, data):\n        self.data = data\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__len__</span>(self):\n        <span class=\"keyword\">return</span> len(self.data)\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__getitem__</span>(self, idx):\n        <span class=\"keyword\">return</span> {\n            <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: torch.tensor(self.data[idx][<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>]),\n            <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: torch.tensor(self.data[idx][<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>]),\n            <span class=\"string\">&quot;</span><span class=\"string\">start_positions</span><span class=\"string\">&quot;</span>: torch.tensor(self.data[idx][<span class=\"string\">&quot;</span><span class=\"string\">start_positions</span><span class=\"string\">&quot;</span>]),\n            <span class=\"string\">&quot;</span><span class=\"string\">end_positions</span><span class=\"string\">&quot;</span>: torch.tensor(self.data[idx][<span class=\"string\">&quot;</span><span class=\"string\">end_positions</span><span class=\"string\">&quot;</span>]),\n        }\n\n\n<span class=\"comment\"># Enhanced training configuration</span>\ntraining_args = TrainingArguments(\n    output_dir=<span class=\"string\">&quot;</span><span class=\"string\">./results</span><span class=\"string\">&quot;</span>,\n    evaluation_strategy=<span class=\"string\">&quot;</span><span class=\"string\">steps</span><span class=\"string\">&quot;</span>,\n    eval_steps=<span class=\"number\">200</span>,\n    learning_rate=<span class=\"number\">2e-5</span>,\n    per_device_train_batch_size=<span class=\"number\">4</span>,\n    per_device_eval_batch_size=<span class=\"number\">4</span>,\n    num_train_epochs=<span class=\"number\">5</span>,\n    weight_decay=<span class=\"number\">0.01</span>,\n    fp16=<span class=\"keyword\">True</span>,\n    gradient_accumulation_steps=<span class=\"number\">2</span>,\n    logging_dir=<span class=\"string\">&quot;</span><span class=\"string\">./logs</span><span class=\"string\">&quot;</span>,\n    report_to=<span class=\"string\">&quot;</span><span class=\"string\">none</span><span class=\"string\">&quot;</span>,\n    save_strategy=<span class=\"string\">&quot;</span><span class=\"string\">no</span><span class=\"string\">&quot;</span>,\n    warmup_ratio=<span class=\"number\">0.1</span>,\n)\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">jaccard</span>(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a &amp; b\n    <span class=\"keyword\">return</span> len(c) / (len(a) + len(b) - len(c)) <span class=\"keyword\">if</span> (a or b) <span class=\"keyword\">else</span> <span class=\"number\">0.0</span>\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">postprocess_answer</span>(context, start, end):\n    answer = context[start:end]\n    <span class=\"keyword\">while</span> len(answer) &gt; <span class=\"number\">0</span> and answer[<span class=\"number\">0</span>] in [<span class=\"string\">&quot;</span><span class=\"string\"> </span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">,</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">.</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">?</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">!</span><span class=\"string\">&quot;</span>]:\n        answer = answer[<span class=\"number\">1</span>:]\n        start += <span class=\"number\">1</span>\n    <span class=\"keyword\">while</span> len(answer) &gt; <span class=\"number\">0</span> and answer[-<span class=\"number\">1</span>] in [<span class=\"string\">&quot;</span><span class=\"string\"> </span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">,</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">.</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">?</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">!</span><span class=\"string\">&quot;</span>]:\n        answer = answer[:-<span class=\"number\">1</span>]\n    <span class=\"keyword\">return</span> answer.strip()\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">predict</span>(contexts, questions):\n    predictions = []\n    model.eval()\n\n    <span class=\"keyword\">for</span> context, question in zip(contexts, questions):\n        encoding = tokenizer(\n            question,\n            context,\n            truncation=<span class=\"string\">&quot;</span><span class=\"string\">only_second</span><span class=\"string\">&quot;</span>,\n            max_length=<span class=\"number\">512</span>,\n            stride=<span class=\"number\">128</span>,\n            return_overflowing_tokens=<span class=\"keyword\">True</span>,\n            return_offsets_mapping=<span class=\"keyword\">True</span>,\n            padding=<span class=\"string\">&quot;</span><span class=\"string\">max_length</span><span class=\"string\">&quot;</span>,\n            return_tensors=<span class=\"string\">&quot;</span><span class=\"string\">pt</span><span class=\"string\">&quot;</span>,\n        ).to(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span>)\n\n        best_score = -float(<span class=\"string\">&quot;</span><span class=\"string\">inf</span><span class=\"string\">&quot;</span>)\n        best_answer = <span class=\"string\">&quot;</span><span class=\"string\">&quot;</span>\n\n        <span class=\"keyword\">for</span> i in range(len(encoding[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>])):\n            <span class=\"keyword\">with</span> torch.no_grad():\n                outputs = model(\n                    input_ids=encoding[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>][i : i + <span class=\"number\">1</span>],\n                    attention_mask=encoding[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>][i : i + <span class=\"number\">1</span>],\n                )\n\n            start_logits = outputs.start_logits[<span class=\"number\">0</span>].cpu().numpy()\n            end_logits = outputs.end_logits[<span class=\"number\">0</span>].cpu().numpy()\n            offset_mapping = encoding[<span class=\"string\">&quot;</span><span class=\"string\">offset_mapping</span><span class=\"string\">&quot;</span>][i].cpu().numpy()\n\n            <span class=\"comment\"># Consider top 20 candidates</span>\n            start_indices = np.argsort(start_logits)[-<span class=\"number\">20</span>:][::-<span class=\"number\">1</span>]\n            end_indices = np.argsort(end_logits)[-<span class=\"number\">20</span>:][::-<span class=\"number\">1</span>]\n\n            <span class=\"keyword\">for</span> start_idx in start_indices:\n                <span class=\"keyword\">for</span> end_idx in end_indices:\n                    <span class=\"keyword\">if</span> start_idx &gt; end_idx:\n                        <span class=\"keyword\">continue</span>\n                    <span class=\"keyword\">if</span> offset_mapping[start_idx][<span class=\"number\">0</span>] &gt; offset_mapping[end_idx][<span class=\"number\">1</span>]:\n                        <span class=\"keyword\">continue</span>\n\n                    answer_start = int(offset_mapping[start_idx][<span class=\"number\">0</span>])\n                    answer_end = int(offset_mapping[end_idx][<span class=\"number\">1</span>])\n                    answer_text = postprocess_answer(context, answer_start, answer_end)\n\n                    <span class=\"keyword\">if</span> not answer_text:\n                        <span class=\"keyword\">continue</span>\n\n                    score = start_logits[start_idx] + end_logits[end_idx]\n                    <span class=\"keyword\">if</span> score &gt; best_score:\n                        best_score = score\n                        best_answer = answer_text\n\n        predictions.append(best_answer <span class=\"keyword\">if</span> best_answer <span class=\"keyword\">else</span> context.split()[<span class=\"number\">0</span>])\n    <span class=\"keyword\">return</span> predictions\n\n\n<span class=\"keyword\">try</span>:\n    train_dataset = QADataset(train_set)\n    val_dataset = QADataset(val_set)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=training_args.per_device_train_batch_size,\n        shuffle=<span class=\"keyword\">True</span>,\n        num_workers=<span class=\"number\">4</span>,\n    )\n    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(len(train_loader) * training_args.warmup_ratio),\n        num_training_steps=len(train_loader) * training_args.num_train_epochs,\n    )\n\n    best_score = <span class=\"number\">0</span>\n    <span class=\"keyword\">for</span> epoch in range(training_args.num_train_epochs):\n        model.train()\n        total_loss = <span class=\"number\">0</span>\n        progress_bar = tqdm(train_loader, desc=<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Epoch </span><span class=\"string\">{</span>epoch+<span class=\"number\">1</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n\n        <span class=\"keyword\">for</span> batch in progress_bar:\n            batch = {k: v.to(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span>) <span class=\"keyword\">for</span> k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n\n            total_loss += loss.item()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class=\"number\">1.0</span>)\n\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            progress_bar.set_postfix(loss=loss.item())\n\n        <span class=\"comment\"># Validation</span>\n        val_orig_indices = list({item[<span class=\"string\">&quot;</span><span class=\"string\">original_idx</span><span class=\"string\">&quot;</span>] <span class=\"keyword\">for</span> item in val_set})\n        val_df = train_df.iloc[val_orig_indices]\n        val_preds = predict(val_df[<span class=\"string\">&quot;</span><span class=\"string\">context</span><span class=\"string\">&quot;</span>].tolist(), val_df[<span class=\"string\">&quot;</span><span class=\"string\">question</span><span class=\"string\">&quot;</span>].tolist())\n        val_score = np.mean(\n            [jaccard(t, p) <span class=\"keyword\">for</span> t, p in zip(val_df[<span class=\"string\">&quot;</span><span class=\"string\">answer_text</span><span class=\"string\">&quot;</span>], val_preds)]\n        )\n        print(\n            <span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Epoch </span><span class=\"string\">{</span>epoch+<span class=\"number\">1</span><span class=\"string\">}</span><span class=\"string\"> | Val Jaccard: </span><span class=\"string\">{</span>val_score<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\"> | Train Loss: </span><span class=\"string\">{</span>total_loss/len(train_loader)<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>\n        )\n\n        <span class=\"keyword\">if</span> val_score &gt; best_score:\n            best_score = val_score\n            torch.save(model.state_dict(), <span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>)\n\n    <span class=\"comment\"># Load best model</span>\n    model.load_state_dict(torch.load(<span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>))\n    test_preds = predict(test_df[<span class=\"string\">&quot;</span><span class=\"string\">context</span><span class=\"string\">&quot;</span>].tolist(), test_df[<span class=\"string\">&quot;</span><span class=\"string\">question</span><span class=\"string\">&quot;</span>].tolist())\n\n<span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:\n    print(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Error: </span><span class=\"string\">{</span>str(e)<span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n    test_preds = [train_df[<span class=\"string\">&quot;</span><span class=\"string\">answer_text</span><span class=\"string\">&quot;</span>].mode()[<span class=\"number\">0</span>]] * len(test_df)\n\n<span class=\"comment\"># Create submission</span>\ntest_df[<span class=\"string\">&quot;</span><span class=\"string\">PredictionString</span><span class=\"string\">&quot;</span>] = test_preds\ntest_df[[<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">PredictionString</span><span class=\"string\">&quot;</span>]].to_csv(<span class=\"string\">&quot;</span><span class=\"string\">./submission/submission.csv</span><span class=\"string\">&quot;</span>, index=<span class=\"keyword\">False</span>)\nprint(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Final Validation Jaccard: </span><span class=\"string\">{</span>best_score<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n"
}