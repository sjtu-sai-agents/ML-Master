{
    "title": "jigsaw-unintended-bias-in-toxicity-classification",
    "steps": [
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> python ml_master.py --task jigsaw-unintended-bias-in-toxicity-classification --time-limit 12h",
            "delay": 200
        },
        {
            "text": "<span class='info'>[INFO]</span>: Starting run \"jigsaw-unintended-bias-in-toxicity-classification\"",
            "delay": 200
        },
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> ",
            "delay": 200
        }
    ],
    "code": "<span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd\n<span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np\n<span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split\n<span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> roc_auc_score\n<span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> DistilBertTokenizer, DistilBertForSequenceClassification\n<span class=\"keyword\">import</span> torch\n<span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader, Dataset\n<span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> BCEWithLogitsLoss\n<span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm\n\n<span class=\"comment\"># Configuration</span>\nMODEL_NAME = <span class=\"string\">&quot;</span><span class=\"string\">distilbert-base-uncased</span><span class=\"string\">&quot;</span>\nBATCH_SIZE = <span class=\"number\">32</span>\nMAX_LEN = <span class=\"number\">256</span>\nEPOCHS = <span class=\"number\">3</span>\nDEVICE = torch.device(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;</span><span class=\"string\">cpu</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Load and preprocess data</span>\ntrain_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/train.csv</span><span class=\"string\">&quot;</span>, usecols=[<span class=\"string\">&quot;</span><span class=\"string\">comment_text</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>])\ntrain_df = train_df.dropna(subset=[<span class=\"string\">&quot;</span><span class=\"string\">comment_text</span><span class=\"string\">&quot;</span>])\ntrain_df[<span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>] = (train_df[<span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>] &gt;= <span class=\"number\">0.5</span>).astype(float)\n\n<span class=\"comment\"># Train-validation split</span>\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df[<span class=\"string\">&quot;</span><span class=\"string\">comment_text</span><span class=\"string\">&quot;</span>].values,\n    train_df[<span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>].values,\n    test_size=<span class=\"number\">0.1</span>,\n    stratify=train_df[<span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>],\n    random_state=<span class=\"number\">42</span>,\n)\n\n<span class=\"comment\"># Tokenization</span>\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\n\n<span class=\"keyword\">class</span> <span class=\"class\">ToxicityDataset</span>(Dataset):\n    <span class=\"keyword\">def</span> <span class=\"function\">__init__</span>(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__len__</span>(self):\n        <span class=\"keyword\">return</span> len(self.texts)\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__getitem__</span>(self, idx):\n        encoding = tokenizer(\n            str(self.texts[idx]),\n            max_length=MAX_LEN,\n            padding=<span class=\"string\">&quot;</span><span class=\"string\">max_length</span><span class=\"string\">&quot;</span>,\n            truncation=<span class=\"keyword\">True</span>,\n            return_tensors=<span class=\"string\">&quot;</span><span class=\"string\">pt</span><span class=\"string\">&quot;</span>,\n        )\n        <span class=\"keyword\">return</span> {\n            <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: encoding[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>].flatten(),\n            <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: encoding[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>].flatten(),\n            <span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>: torch.tensor(self.labels[idx], dtype=torch.float),\n        }\n\n\ntrain_dataset = ToxicityDataset(train_texts, train_labels)\nval_dataset = ToxicityDataset(val_texts, val_labels)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">4</span>\n)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=<span class=\"number\">4</span>)\n\n<span class=\"comment\"># Model setup</span>\nmodel = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=<span class=\"number\">1</span>)\nmodel.to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=<span class=\"number\">2e-5</span>)\nloss_fn = BCEWithLogitsLoss(pos_weight=torch.tensor([<span class=\"number\">5.0</span>]).to(DEVICE))\n\n<span class=\"comment\"># Training loop</span>\nbest_val_auc = <span class=\"number\">0</span>\n<span class=\"keyword\">for</span> epoch in range(EPOCHS):\n    model.train()\n    <span class=\"keyword\">for</span> batch in tqdm(train_loader, desc=<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Epoch </span><span class=\"string\">{</span>epoch+<span class=\"number\">1</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>):\n        optimizer.zero_grad()\n        inputs = {\n            <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>].to(DEVICE),\n            <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>].to(DEVICE),\n            <span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>].to(DEVICE),\n        }\n        outputs = model(**inputs)\n        loss = loss_fn(outputs.logits.squeeze(), inputs[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>])\n        loss.backward()\n        optimizer.step()\n\n    <span class=\"comment\"># Validation</span>\n    model.eval()\n    val_preds = []\n    val_true = []\n    <span class=\"keyword\">with</span> torch.no_grad():\n        <span class=\"keyword\">for</span> batch in val_loader:\n            inputs = {\n                <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>].to(DEVICE),\n                <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>].to(DEVICE),\n            }\n            outputs = model(**inputs).logits.squeeze().cpu().numpy()\n            val_preds.extend(outputs)\n            val_true.extend(batch[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>].numpy())\n\n    val_auc = roc_auc_score(val_true, val_preds)\n    print(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Validation AUC: </span><span class=\"string\">{</span>val_auc<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n    <span class=\"keyword\">if</span> val_auc &gt; best_val_auc:\n        best_val_auc = val_auc\n        torch.save(model.state_dict(), <span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Generate predictions</span>\ntest_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/test.csv</span><span class=\"string\">&quot;</span>)\ntest_dataset = ToxicityDataset(test_df[<span class=\"string\">&quot;</span><span class=\"string\">comment_text</span><span class=\"string\">&quot;</span>].values, np.zeros(len(test_df)))\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=<span class=\"number\">4</span>)\n\nmodel.load_state_dict(torch.load(<span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>))\nmodel.eval()\ntest_preds = []\n<span class=\"keyword\">with</span> torch.no_grad():\n    <span class=\"keyword\">for</span> batch in tqdm(test_loader):\n        inputs = {\n            <span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>].to(DEVICE),\n            <span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>: batch[<span class=\"string\">&quot;</span><span class=\"string\">attention_mask</span><span class=\"string\">&quot;</span>].to(DEVICE),\n        }\n        outputs = model(**inputs).logits.squeeze().cpu().numpy()\n        test_preds.extend(outputs)\n\n<span class=\"comment\"># Save submission</span>\nsubmission = pd.DataFrame({<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>: test_df[<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>], <span class=\"string\">&quot;</span><span class=\"string\">prediction</span><span class=\"string\">&quot;</span>: test_preds})\nsubmission.to_csv(<span class=\"string\">&quot;</span><span class=\"string\">./submission/submission.csv</span><span class=\"string\">&quot;</span>, index=<span class=\"keyword\">False</span>)\n\nprint(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Final Validation AUC: </span><span class=\"string\">{</span>best_val_auc<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n"
}