{
    "title": "spooky-author-identification",
    "steps": [
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> python ml_master.py --task spooky-author-identification --time-limit 12h",
            "delay": 200
        },
        {
            "text": "<span class='info'>[INFO]</span>: Starting run \"spooky-author-identification\"",
            "delay": 200
        },
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> ",
            "delay": 200
        }
    ],
    "code": "<span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd\n<span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np\n<span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split\n<span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> log_loss\n<span class=\"keyword\">import</span> torch\n<span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F\n<span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader, Dataset\n<span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup,\n)\n<span class=\"keyword\">from</span> torch.optim <span class=\"keyword\">import</span> AdamW\n<span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm\n<span class=\"keyword\">from</span> torch.cuda.amp <span class=\"keyword\">import</span> autocast, GradScaler\n\ndevice = torch.device(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;</span><span class=\"string\">cpu</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Load data</span>\ntrain_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/train.csv</span><span class=\"string\">&quot;</span>)\ntest_df = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">./input/test.csv</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Split validation set</span>\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df[<span class=\"string\">&quot;</span><span class=\"string\">text</span><span class=\"string\">&quot;</span>].values,\n    train_df[<span class=\"string\">&quot;</span><span class=\"string\">author</span><span class=\"string\">&quot;</span>].values,\n    test_size=<span class=\"number\">0.2</span>,\n    stratify=train_df[<span class=\"string\">&quot;</span><span class=\"string\">author</span><span class=\"string\">&quot;</span>],\n    random_state=<span class=\"number\">42</span>,\n)\n\n<span class=\"comment\"># Compute class weights</span>\nauthor_map = {<span class=\"string\">&quot;</span><span class=\"string\">EAP</span><span class=\"string\">&quot;</span>: <span class=\"number\">0</span>, <span class=\"string\">&quot;</span><span class=\"string\">HPL</span><span class=\"string\">&quot;</span>: <span class=\"number\">1</span>, <span class=\"string\">&quot;</span><span class=\"string\">MWS</span><span class=\"string\">&quot;</span>: <span class=\"number\">2</span>}\ny_train_mapped = np.array([author_map[a] <span class=\"keyword\">for</span> a in y_train])\nunique, counts = np.unique(y_train_mapped, return_counts=<span class=\"keyword\">True</span>)\nclass_weights = (<span class=\"number\">1.0</span> / (counts / len(y_train_mapped))).astype(np.float32)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n<span class=\"comment\"># Model configuration</span>\nmodel_name = <span class=\"string\">&quot;</span><span class=\"string\">microsoft/deberta-v3-large</span><span class=\"string\">&quot;</span>\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_length = <span class=\"number\">192</span>\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">tokenize</span>(texts):\n    <span class=\"keyword\">return</span> tokenizer(\n        texts.tolist(),\n        max_length=max_length,\n        padding=<span class=\"string\">&quot;</span><span class=\"string\">max_length</span><span class=\"string\">&quot;</span>,\n        truncation=<span class=\"keyword\">True</span>,\n        return_tensors=<span class=\"string\">&quot;</span><span class=\"string\">pt</span><span class=\"string\">&quot;</span>,\n    )\n\n\ntrain_encodings = tokenize(X_train)\nval_encodings = tokenize(X_val)\ntest_encodings = tokenize(test_df[<span class=\"string\">&quot;</span><span class=\"string\">text</span><span class=\"string\">&quot;</span>])\n\n\n<span class=\"keyword\">class</span> <span class=\"class\">AuthorDataset</span>(Dataset):\n    <span class=\"keyword\">def</span> <span class=\"function\">__init__</span>(self, encodings, labels=<span class=\"keyword\">None</span>):\n        self.encodings = encodings\n        self.labels = labels\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__getitem__</span>(self, idx):\n        item = {k: v[idx] <span class=\"keyword\">for</span> k, v in self.encodings.items()}\n        <span class=\"keyword\">if</span> self.labels is not <span class=\"keyword\">None</span>:\n            item[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>] = torch.tensor(self.labels[idx])\n        <span class=\"keyword\">return</span> item\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__len__</span>(self):\n        <span class=\"keyword\">return</span> len(self.encodings[<span class=\"string\">&quot;</span><span class=\"string\">input_ids</span><span class=\"string\">&quot;</span>])\n\n\ntrain_dataset = AuthorDataset(train_encodings, [author_map[a] <span class=\"keyword\">for</span> a in y_train])\nval_dataset = AuthorDataset(val_encodings, [author_map[a] <span class=\"keyword\">for</span> a in y_val])\ntest_dataset = AuthorDataset(test_encodings)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=<span class=\"number\">3</span>).to(\n    device\n)\noptimizer = AdamW(model.parameters(), lr=<span class=\"number\">2e-5</span>, weight_decay=<span class=\"number\">1e-5</span>)\ntrain_loader = DataLoader(train_dataset, batch_size=<span class=\"number\">8</span>, shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">4</span>)\nval_loader = DataLoader(val_dataset, batch_size=<span class=\"number\">16</span>, num_workers=<span class=\"number\">4</span>)\ntest_loader = DataLoader(test_dataset, batch_size=<span class=\"number\">16</span>, num_workers=<span class=\"number\">4</span>)\n\nscaler = GradScaler()\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=<span class=\"number\">100</span>, num_training_steps=len(train_loader) * <span class=\"number\">5</span>\n)\n\n<span class=\"comment\"># Training loop with class weights</span>\nbest_val_loss = float(<span class=\"string\">&quot;</span><span class=\"string\">inf</span><span class=\"string\">&quot;</span>)\n<span class=\"keyword\">for</span> epoch in range(<span class=\"number\">5</span>):\n    model.train()\n    <span class=\"keyword\">for</span> batch in tqdm(train_loader, desc=<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Epoch </span><span class=\"string\">{</span>epoch<span class=\"string\">}</span><span class=\"string\">&quot;</span>):\n        optimizer.zero_grad()\n        inputs = {k: v.to(device) <span class=\"keyword\">for</span> k, v in batch.items() <span class=\"keyword\">if</span> k != <span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>}\n        labels = batch[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>].to(device)\n\n        <span class=\"keyword\">with</span> autocast():\n            outputs = model(**inputs)\n            loss = F.cross_entropy(outputs.logits, labels, weight=class_weights)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n    <span class=\"comment\"># Validation</span>\n    model.eval()\n    val_probs, val_true = [], []\n    <span class=\"keyword\">with</span> torch.no_grad():\n        <span class=\"keyword\">for</span> batch in val_loader:\n            inputs = {k: v.to(device) <span class=\"keyword\">for</span> k, v in batch.items() <span class=\"keyword\">if</span> k != <span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>}\n            logits = model(**inputs).logits\n            val_probs.extend(torch.softmax(logits, <span class=\"number\">1</span>).cpu().numpy())\n            val_true.extend(batch[<span class=\"string\">&quot;</span><span class=\"string\">labels</span><span class=\"string\">&quot;</span>].numpy())\n\n    val_loss = log_loss(val_true, np.clip(val_probs, <span class=\"number\">1e-15</span>, <span class=\"number\">1</span> - <span class=\"number\">1e-15</span>))\n    print(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Validation Loss: </span><span class=\"string\">{</span>val_loss<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n\n    <span class=\"keyword\">if</span> val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), <span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>)\n    <span class=\"keyword\">else</span>:\n        <span class=\"keyword\">break</span>\n\n<span class=\"comment\"># Generate submission</span>\nmodel.load_state_dict(torch.load(<span class=\"string\">&quot;</span><span class=\"string\">./working/best_model.pt</span><span class=\"string\">&quot;</span>))\nmodel.eval()\ntest_probs = []\n<span class=\"keyword\">with</span> torch.no_grad():\n    <span class=\"keyword\">for</span> batch in test_loader:\n        inputs = {k: v.to(device) <span class=\"keyword\">for</span> k, v in batch.items()}\n        logits = model(**inputs).logits\n        test_probs.extend(torch.softmax(logits, <span class=\"number\">1</span>).cpu().numpy())\n\nsubmission = pd.DataFrame(\n    {\n        <span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>: test_df[<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>],\n        <span class=\"string\">&quot;</span><span class=\"string\">EAP</span><span class=\"string\">&quot;</span>: [p[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> p in test_probs],\n        <span class=\"string\">&quot;</span><span class=\"string\">HPL</span><span class=\"string\">&quot;</span>: [p[<span class=\"number\">1</span>] <span class=\"keyword\">for</span> p in test_probs],\n        <span class=\"string\">&quot;</span><span class=\"string\">MWS</span><span class=\"string\">&quot;</span>: [p[<span class=\"number\">2</span>] <span class=\"keyword\">for</span> p in test_probs],\n    }\n)\nsubmission[[<span class=\"string\">&quot;</span><span class=\"string\">EAP</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">HPL</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">MWS</span><span class=\"string\">&quot;</span>]] = submission[[<span class=\"string\">&quot;</span><span class=\"string\">EAP</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">HPL</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">MWS</span><span class=\"string\">&quot;</span>]].clip(\n    <span class=\"number\">1e-15</span>, <span class=\"number\">1</span> - <span class=\"number\">1e-15</span>\n)\nsubmission.to_csv(<span class=\"string\">&quot;</span><span class=\"string\">./submission/submission.csv</span><span class=\"string\">&quot;</span>, index=<span class=\"keyword\">False</span>)\nprint(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Final Validation Log Loss: </span><span class=\"string\">{</span>best_val_loss<span class=\"string\">:</span><span class=\"string\">.4f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n"
}