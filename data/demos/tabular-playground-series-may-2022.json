{
    "title": "tabular-playground-series-may-2022",
    "steps": [
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> python ml_master.py --task tabular-playground-series-may-2022 --time-limit 12h",
            "delay": 200
        },
        {
            "text": "<span class='info'>[INFO]</span>: Starting run \"tabular-playground-series-may-2022\"",
            "delay": 200
        },
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> ",
            "delay": 200
        }
    ],
    "code": "<span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd\n<span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np\n<span class=\"keyword\">from</span> scipy.stats <span class=\"keyword\">import</span> entropy\n<span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split\n<span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler\n<span class=\"keyword\">import</span> torch\n<span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn\n<span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader, Dataset\n<span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> roc_auc_score\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">calculate_entropy</span>(s):\n    counts = [s.count(c) <span class=\"keyword\">for</span> c in set(s)]\n    prob = np.array(counts) / len(s)\n    <span class=\"keyword\">return</span> entropy(prob)\n\n\n<span class=\"comment\"># Load data</span>\ntrain = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">input/train.csv</span><span class=\"string\">&quot;</span>)\ntest = pd.read_csv(<span class=\"string\">&quot;</span><span class=\"string\">input/test.csv</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Split f_27 into character features</span>\n<span class=\"keyword\">for</span> i in range(<span class=\"number\">10</span>):\n    train[<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f27_</span><span class=\"string\">{</span>i<span class=\"string\">}</span><span class=\"string\">&quot;</span>] = train[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].str[i]\n    test[<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f27_</span><span class=\"string\">{</span>i<span class=\"string\">}</span><span class=\"string\">&quot;</span>] = test[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].str[i]\n\n<span class=\"comment\"># Add new engineered features</span>\n<span class=\"keyword\">for</span> df in [train, test]:\n    <span class=\"keyword\">for</span> letter in <span class=\"string\">&quot;</span><span class=\"string\">ABCDEFGHIJKLMNOPQRSTUVWXYZ</span><span class=\"string\">&quot;</span>:\n        df[<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f27_count_</span><span class=\"string\">{</span>letter<span class=\"string\">}</span><span class=\"string\">&quot;</span>] = df[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].str.count(letter)\n    df[<span class=\"string\">&quot;</span><span class=\"string\">f27_unique_count</span><span class=\"string\">&quot;</span>] = df[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].apply(<span class=\"keyword\">lambda</span> s: len(set(s)))\n    df[<span class=\"string\">&quot;</span><span class=\"string\">f27_max_count</span><span class=\"string\">&quot;</span>] = df[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].apply(<span class=\"keyword\">lambda</span> s: max([s.count(c) <span class=\"keyword\">for</span> c in s]))\n    df[<span class=\"string\">&quot;</span><span class=\"string\">f27_entropy</span><span class=\"string\">&quot;</span>] = df[<span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>].apply(calculate_entropy)\n\n<span class=\"comment\"># Define features</span>\ncategorical = (\n    [<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f_</span><span class=\"string\">{</span>i<span class=\"string\">:</span><span class=\"string\">02d</span><span class=\"string\">}</span><span class=\"string\">&quot;</span> <span class=\"keyword\">for</span> i in range(<span class=\"number\">7</span>, <span class=\"number\">19</span>)]\n    + [<span class=\"string\">&quot;</span><span class=\"string\">f_29</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">f_30</span><span class=\"string\">&quot;</span>]\n    + [<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f27_</span><span class=\"string\">{</span>i<span class=\"string\">}</span><span class=\"string\">&quot;</span> <span class=\"keyword\">for</span> i in range(<span class=\"number\">10</span>)]\n)\nnumerical = (\n    [<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f_</span><span class=\"string\">{</span>i<span class=\"string\">:</span><span class=\"string\">02d</span><span class=\"string\">}</span><span class=\"string\">&quot;</span> <span class=\"keyword\">for</span> i in range(<span class=\"number\">7</span>)]\n    + [<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f_</span><span class=\"string\">{</span>i<span class=\"string\">}</span><span class=\"string\">&quot;</span> <span class=\"keyword\">for</span> i in range(<span class=\"number\">19</span>, <span class=\"number\">27</span>)]\n    + [<span class=\"string\">&quot;</span><span class=\"string\">f_28</span><span class=\"string\">&quot;</span>]\n    + [<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">f27_count_</span><span class=\"string\">{</span>letter<span class=\"string\">}</span><span class=\"string\">&quot;</span> <span class=\"keyword\">for</span> letter in <span class=\"string\">&quot;</span><span class=\"string\">ABCDEFGHIJKLMNOPQRSTUVWXYZ</span><span class=\"string\">&quot;</span>]\n    + [<span class=\"string\">&quot;</span><span class=\"string\">f27_unique_count</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">f27_max_count</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">f27_entropy</span><span class=\"string\">&quot;</span>]\n)\n\n<span class=\"comment\"># Process categorical features</span>\n<span class=\"keyword\">for</span> col in categorical:\n    train_vals = train[col].unique()\n    mapping = {v: i + <span class=\"number\">1</span> <span class=\"keyword\">for</span> i, v in enumerate(train_vals)}\n    train[col] = train[col].map(mapping).fillna(<span class=\"number\">0</span>).astype(int)\n    test[col] = test[col].map(mapping).fillna(<span class=\"number\">0</span>).astype(int)\n\n<span class=\"comment\"># Process numerical features</span>\nscaler = StandardScaler()\ntrain[numerical] = scaler.fit_transform(train[numerical])\ntest[numerical] = scaler.transform(test[numerical])\n\n<span class=\"comment\"># Split data</span>\nX = train.drop([<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">f_27</span><span class=\"string\">&quot;</span>], axis=<span class=\"number\">1</span>)\ny = train[<span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>]\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=<span class=\"number\">0.2</span>, stratify=y, random_state=<span class=\"number\">42</span>\n)\n\n\n<span class=\"keyword\">class</span> <span class=\"class\">TabDataset</span>(Dataset):\n    <span class=\"keyword\">def</span> <span class=\"function\">__init__</span>(self, cat, num, y=<span class=\"keyword\">None</span>):\n        self.cat = torch.tensor(cat.values, dtype=torch.long)\n        self.num = torch.tensor(num.values, dtype=torch.float)\n        self.y = torch.tensor(y.values, dtype=torch.float) <span class=\"keyword\">if</span> y is not <span class=\"keyword\">None</span> <span class=\"keyword\">else</span> <span class=\"keyword\">None</span>\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__len__</span>(self):\n        <span class=\"keyword\">return</span> len(self.cat)\n\n    <span class=\"keyword\">def</span> <span class=\"function\">__getitem__</span>(self, idx):\n        <span class=\"keyword\">if</span> self.y is not <span class=\"keyword\">None</span>:\n            <span class=\"keyword\">return</span> self.cat[idx], self.num[idx], self.y[idx]\n        <span class=\"keyword\">return</span> self.cat[idx], self.num[idx]\n\n\ntrain_set = TabDataset(X_train[categorical], X_train[numerical], y_train)\nval_set = TabDataset(X_val[categorical], X_val[numerical], y_val)\n\n\n<span class=\"keyword\">class</span> <span class=\"class\">NeuralNet</span>(nn.Module):\n    <span class=\"keyword\">def</span> <span class=\"function\">__init__</span>(self, emb_sizes, num_size):\n        super().<span class=\"function\">__init__</span>()\n        self.embs = nn.ModuleList([nn.Embedding(n, <span class=\"number\">4</span>) <span class=\"keyword\">for</span> n, _ in emb_sizes])\n        self.num_norm = nn.BatchNorm1d(num_size)\n        self.layers = nn.Sequential(\n            nn.Linear(sum(<span class=\"number\">4</span> <span class=\"keyword\">for</span> _ in emb_sizes) + num_size + <span class=\"number\">1</span>, <span class=\"number\">512</span>),\n            nn.BatchNorm1d(<span class=\"number\">512</span>),\n            nn.ReLU(),\n            nn.Dropout(<span class=\"number\">0.3</span>),\n            nn.Linear(<span class=\"number\">512</span>, <span class=\"number\">256</span>),\n            nn.ReLU(),\n            nn.Dropout(<span class=\"number\">0.2</span>),\n            nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">1</span>),\n        )\n\n    <span class=\"keyword\">def</span> <span class=\"function\">forward</span>(self, cat, num):\n        num = self.num_norm(num)\n        embeddings = [e(cat[:, i]) <span class=\"keyword\">for</span> i, e in enumerate(self.embs)]\n        stacked = torch.stack(embeddings, dim=<span class=\"number\">1</span>)\n\n        <span class=\"comment\"># Factorization Machine component</span>\n        sum_emb = torch.sum(stacked, dim=<span class=\"number\">1</span>)\n        sum_square = torch.sum(sum_emb**<span class=\"number\">2</span>, dim=<span class=\"number\">1</span>)\n        square_sum = torch.sum(stacked**<span class=\"number\">2</span>, dim=(<span class=\"number\">1</span>, <span class=\"number\">2</span>))\n        fm_term = <span class=\"number\">0.5</span> * (sum_square - square_sum).unsqueeze(<span class=\"number\">1</span>)\n\n        x = torch.cat(\n            [emb.flatten(start_dim=<span class=\"number\">1</span>) <span class=\"keyword\">for</span> emb in embeddings] + [num, fm_term], dim=<span class=\"number\">1</span>\n        )\n        <span class=\"keyword\">return</span> self.layers(x)\n\n\nemb_sizes = [(train[c].max() + <span class=\"number\">1</span>, <span class=\"number\">4</span>) <span class=\"keyword\">for</span> c in categorical]\nmodel = NeuralNet(emb_sizes, len(numerical)).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=<span class=\"number\">0.001</span>)\ncriterion = nn.BCEWithLogitsLoss()\n\ntrain_loader = DataLoader(train_set, batch_size=<span class=\"number\">2048</span>, shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">4</span>)\nval_loader = DataLoader(val_set, batch_size=<span class=\"number\">4096</span>, num_workers=<span class=\"number\">4</span>)\n\nbest_auc, patience = <span class=\"number\">0</span>, <span class=\"number\">5</span>\n<span class=\"keyword\">for</span> epoch in range(<span class=\"number\">30</span>):\n    model.train()\n    <span class=\"keyword\">for</span> cat, num, y in train_loader:\n        cat, num, y = cat.cuda(), num.cuda(), y.cuda()\n        optimizer.zero_grad()\n        loss = criterion(model(cat, num).squeeze(), y)\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    preds, truths = [], []\n    <span class=\"keyword\">with</span> torch.no_grad():\n        <span class=\"keyword\">for</span> cat, num, y in val_loader:\n            p = torch.sigmoid(model(cat.cuda(), num.cuda()).squeeze()).cpu()\n            preds.append(p.numpy())\n            truths.append(y.numpy())\n    auc = roc_auc_score(np.concatenate(truths), np.concatenate(preds))\n    print(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Epoch </span><span class=\"string\">{</span>epoch+<span class=\"number\">1</span><span class=\"string\">}</span><span class=\"string\">: AUC </span><span class=\"string\">{</span>auc<span class=\"string\">:</span><span class=\"string\">.5f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n\n    <span class=\"keyword\">if</span> auc &gt; best_auc:\n        best_auc = auc\n        torch.save(model.state_dict(), <span class=\"string\">&quot;</span><span class=\"string\">best.pt</span><span class=\"string\">&quot;</span>)\n        patience = <span class=\"number\">5</span>\n    <span class=\"keyword\">else</span>:\n        patience -= <span class=\"number\">1</span>\n        <span class=\"keyword\">if</span> patience == <span class=\"number\">0</span>:\n            <span class=\"keyword\">break</span>\n\n<span class=\"comment\"># Generate predictions</span>\nmodel.load_state_dict(torch.load(<span class=\"string\">&quot;</span><span class=\"string\">best.pt</span><span class=\"string\">&quot;</span>))\ntest_set = TabDataset(test[categorical], test[numerical])\ntest_loader = DataLoader(test_set, batch_size=<span class=\"number\">4096</span>, num_workers=<span class=\"number\">4</span>)\n\nmodel.eval()\npreds = []\n<span class=\"keyword\">with</span> torch.no_grad():\n    <span class=\"keyword\">for</span> cat, num in test_loader:\n        p = torch.sigmoid(model(cat.cuda(), num.cuda()).squeeze()).cpu()\n        preds.append(p.numpy())\n\npd.DataFrame({<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>: test.id, <span class=\"string\">&quot;</span><span class=\"string\">target</span><span class=\"string\">&quot;</span>: np.concatenate(preds)}).to_csv(\n    <span class=\"string\">&quot;</span><span class=\"string\">submission/submission.csv</span><span class=\"string\">&quot;</span>, index=<span class=\"keyword\">False</span>\n)\nprint(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Final Validation AUC: </span><span class=\"string\">{</span>best_auc<span class=\"string\">:</span><span class=\"string\">.5f</span><span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n"
}