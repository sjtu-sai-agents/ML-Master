{
    "title": "billion-word-imputation",
    "steps": [
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> python ml_master.py --task billion-word-imputation --time-limit 12h",
            "delay": 200
        },
        {
            "text": "<span class='info'>[INFO]</span>: Starting run \"billion-word-imputation\"",
            "delay": 200
        },
        {
            "text": "<span class='prompt'>ml-master@ai4ai:~$</span> ",
            "delay": 200
        }
    ],
    "code": "<span class=\"keyword\">import</span> torch\n<span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> BertForMaskedLM, BertTokenizerFast\n<span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd\n<span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm\n<span class=\"keyword\">import</span> editdistance\n\n<span class=\"comment\"># Use faster tokenizer and smaller model for efficiency</span>\nmodel_name = <span class=\"string\">&quot;</span><span class=\"string\">bert-base-uncased</span><span class=\"string\">&quot;</span>\ntokenizer = BertTokenizerFast.from_pretrained(model_name)\nmodel = BertForMaskedLM.from_pretrained(model_name)\nmodel.to(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span>)\nmodel.eval()\n\n\n<span class=\"keyword\">def</span> <span class=\"function\">predict_missing_word</span>(sentence):\n    words = sentence.split()\n    <span class=\"keyword\">if</span> len(words) &lt; <span class=\"number\">2</span>:\n        <span class=\"keyword\">return</span> sentence\n\n    <span class=\"comment\"># Generate all possible masked positions</span>\n    masked_sentences = []\n    positions = []\n    <span class=\"keyword\">for</span> i in range(<span class=\"number\">1</span>, len(words)):\n        masked = words[:i] + [<span class=\"string\">&quot;</span><span class=\"string\">[MASK]</span><span class=\"string\">&quot;</span>] + words[i:]\n        masked_sentences.append(<span class=\"string\">&quot;</span><span class=\"string\"> </span><span class=\"string\">&quot;</span>.join(masked))\n        positions.append(i)\n\n    <span class=\"keyword\">if</span> not masked_sentences:\n        <span class=\"keyword\">return</span> sentence\n\n    <span class=\"comment\"># Process all masks in one batch</span>\n    inputs = tokenizer(\n        masked_sentences,\n        padding=<span class=\"keyword\">True</span>,\n        truncation=<span class=\"keyword\">True</span>,\n        max_length=<span class=\"number\">128</span>,\n        return_tensors=<span class=\"string\">&quot;</span><span class=\"string\">pt</span><span class=\"string\">&quot;</span>,\n    ).to(<span class=\"string\">&quot;</span><span class=\"string\">cuda</span><span class=\"string\">&quot;</span>)\n\n    <span class=\"keyword\">with</span> torch.no_grad():\n        outputs = model(**inputs)\n\n    <span class=\"comment\"># Find best prediction for each mask position</span>\n    best_score = -float(<span class=\"string\">&quot;</span><span class=\"string\">inf</span><span class=\"string\">&quot;</span>)\n    best_prediction = sentence\n    mask_token_id = tokenizer.mask_token_id\n\n    <span class=\"keyword\">for</span> idx, (logits, pos) in enumerate(zip(outputs.logits, positions)):\n        mask_idx = (inputs.input_ids[idx] == mask_token_id).nonzero(as_tuple=<span class=\"keyword\">True</span>)[<span class=\"number\">0</span>]\n        <span class=\"keyword\">if</span> mask_idx.numel() == <span class=\"number\">0</span>:\n            <span class=\"keyword\">continue</span>\n        mask_idx = mask_idx[<span class=\"number\">0</span>]\n        probs = torch.softmax(logits[mask_idx], dim=-<span class=\"number\">1</span>)\n        top_token_id = probs.argmax()\n        score = probs[top_token_id].item()\n\n        <span class=\"keyword\">if</span> score &gt; best_score:\n            predicted_word = tokenizer.decode(top_token_id, skip_special_tokens=<span class=\"keyword\">True</span>)\n            new_sentence = words[:pos] + [predicted_word] + words[pos:]\n            best_score = score\n            best_prediction = <span class=\"string\">&quot;</span><span class=\"string\"> </span><span class=\"string\">&quot;</span>.join(new_sentence)\n\n    <span class=\"keyword\">return</span> best_prediction\n\n\n<span class=\"comment\"># Validation using corrected file path</span>\nval_size = <span class=\"number\">50</span>\nval_original = []\nval_corrupted = []\n<span class=\"keyword\">with</span> open(<span class=\"string\">&quot;</span><span class=\"string\">input/train_v2.txt</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">r</span><span class=\"string\">&quot;</span>, encoding=<span class=\"string\">&quot;</span><span class=\"string\">utf-8</span><span class=\"string\">&quot;</span>) <span class=\"keyword\">as</span> f:\n    <span class=\"keyword\">for</span> _ in range(<span class=\"number\">1000</span>):\n        line = f.readline().strip()\n        <span class=\"keyword\">if</span> not line:\n            <span class=\"keyword\">break</span>\n        words = line.split()\n        <span class=\"keyword\">if</span> len(words) &lt; <span class=\"number\">3</span>:\n            <span class=\"keyword\">continue</span>\n        remove_pos = torch.randint(<span class=\"number\">1</span>, len(words) - <span class=\"number\">1</span>, (<span class=\"number\">1</span>,)).item()\n        corrupted = <span class=\"string\">&quot;</span><span class=\"string\"> </span><span class=\"string\">&quot;</span>.join(words[:remove_pos] + words[remove_pos + <span class=\"number\">1</span> :])\n        val_original.append(line)\n        val_corrupted.append(corrupted)\n        <span class=\"keyword\">if</span> len(val_original) &gt;= val_size:\n            <span class=\"keyword\">break</span>\n\nval_preds = [predict_missing_word(s) <span class=\"keyword\">for</span> s in tqdm(val_corrupted)]\nmean_dist = (\n    sum(editdistance.eval(o, p) <span class=\"keyword\">for</span> o, p in zip(val_original, val_preds)) / val_size\n)\nprint(<span class=\"string\">f</span><span class=\"string\">&quot;</span><span class=\"string\">Validation Mean Levenshtein Distance: </span><span class=\"string\">{</span>mean_dist<span class=\"string\">}</span><span class=\"string\">&quot;</span>)\n\n<span class=\"comment\"># Process test data in batches</span>\ntest_sentences = []\n<span class=\"keyword\">with</span> open(<span class=\"string\">&quot;</span><span class=\"string\">input/test_v2.txt</span><span class=\"string\">&quot;</span>, <span class=\"string\">&quot;</span><span class=\"string\">r</span><span class=\"string\">&quot;</span>, encoding=<span class=\"string\">&quot;</span><span class=\"string\">utf-8</span><span class=\"string\">&quot;</span>) <span class=\"keyword\">as</span> f:\n    test_sentences = [line.strip() <span class=\"keyword\">for</span> line in f]\n\nsubmission = []\n<span class=\"keyword\">for</span> idx, sent in tqdm(enumerate(test_sentences), total=len(test_sentences)):\n    processed = predict_missing_word(sent)\n    submission.append({<span class=\"string\">&quot;</span><span class=\"string\">id</span><span class=\"string\">&quot;</span>: idx + <span class=\"number\">1</span>, <span class=\"string\">&quot;</span><span class=\"string\">sentence</span><span class=\"string\">&quot;</span>: processed})\n\npd.DataFrame(submission).to_csv(<span class=\"string\">&quot;</span><span class=\"string\">submission/submission.csv</span><span class=\"string\">&quot;</span>, index=<span class=\"keyword\">False</span>, quoting=<span class=\"number\">1</span>)\n"
}